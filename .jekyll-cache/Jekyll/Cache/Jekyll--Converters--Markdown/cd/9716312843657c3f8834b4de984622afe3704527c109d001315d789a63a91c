I"9<blockquote>
  <p>Title : EfficientNet, Rethinking Model Scaling for Convolutional Neural Network</p>
</blockquote>

<blockquote>
  <p>Paper link : <a href="https://arxiv.org/pdf/1905.11946.pdf">https://arxiv.org/pdf/1905.11946.pdf</a></p>
</blockquote>

<blockquote>
  <p>Publised year : 23 Nov 2019</p>
</blockquote>

<blockquote>
  <p>keywords : Model Scaling, Classification</p>
</blockquote>

<hr />

<blockquote>
  <p>In this paper, we systematically study model scaling and identify that carefully <strong>balancing network</strong> depth, width, and resolution can lead to better performance. Based on this observation, we propose <strong>a new scaling method that uniformly scales all dimensions of depth/width/resolution</strong> using a simple yet highly effective <strong>compound coefficient</strong>.</p>
</blockquote>

<p><img src="https://drive.google.com/uc?id=1tgBVYooCbdxLcHi6eByFyXQfEGMVPKY7" alt="Model Scaling" width="100%" height="100%" /></p>

<p><br /></p>
<h2 id="introduction">Introduction</h2>
<hr />

<blockquote>
  <p>In previous work, it is common to scale only one of the three dimensions â€“ depth, width, and image size. Though it is possible to scale two or three dimensions arbitrarily, arbitrary scaling requires tedious manual tuning and still often yields sub-optimal accuracy and efficiency. â€¦  In particular, we investigate the central question: is there a principled method to scale up ConvNets that can achieve better accuracy and efficiency?</p>
</blockquote>

<ul>
  <li>ê¸°ì¡´ ì—°êµ¬ì—ì„œëŠ” ì„±ëŠ¥ì„ ì˜¬ë¦¬ê¸° ìœ„í•´ depth, width, image size ì¤‘ í•˜ë‚˜ë§Œ ì¡°ì ˆí•˜ì—¬ ìˆ˜ë™ìœ¼ë¡œ íŠœë‹ì„ í•˜ì˜€ë‹¤. í•˜ì§€ë§Œ ì´ëŸ¬í•œ ë°©ë²•ìœ¼ë¡œëŠ” ìµœì ê°’ì„ ì°¾ê¸° ì–´ë µë‹¤. ì €ìë“¤ì€ <span style="background-color:#BFFF00">â€œ3ê°€ì§€ ìš”ì†Œë¥¼ ì ì ˆí•˜ê²Œ ì¡°ì ˆí•˜ì—¬ ì„±ëŠ¥ì„ ìµœì í™”í•˜ëŠ” ë°©ë²•ì´ ì—†ì„ê¹Œ?â€</span> ë¼ëŠ” ì§ˆë¬¸ì—ì„œ ë¶€í„° ì‹œì‘í•˜ì—¬ ë…¼ì§€ë¥¼ ì „ê°œí•œë‹¤.</li>
</ul>

<blockquote>
  <p>Unlike conventional practice that arbitrary scales these factors, our method uniformly scales network width, depth, and resolution with a set of fixed scaling coefficients.</p>
</blockquote>

<ul>
  <li>ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ê·¸ëŸ° ë°©ë²•ì˜ ì¼í™˜ìœ¼ë¡œ ì„ì˜ì˜ ê°’ì´ ì•„ë‹Œ, ê³ ì •ëœ scaling coefficientsë¡œ ë™ì¼í•˜ê²Œ ì ìš©í•  ìˆ˜ ìˆëŠ” <strong>â€œCompound scaling methodâ€</strong>ë¥¼ ì†Œê°œí•œë‹¤.</li>
  <li>ImageNet competitionì—ì„œ <a href="https://arxiv.org/abs/1811.06965">GPipe(Huang et al.,2018)</a>ëŠ” 84.3%ì˜ ê°€ì¥ ë†’ì€ accuracyë¥¼ ë³´ì˜€ì§€ë§Œ, íŒŒë¼ë¯¸í„°ê°€ 556Mìœ¼ë¡œ ë§ì€ ë©”ëª¨ë¦¬ë¥¼ ì°¨ì§€í•˜ê²Œ ëœë‹¤ëŠ” ë‹¨ì ì´ ì¡´ì¬í•œë‹¤.</li>
  <li>ImageNetìœ¼ë¡œ í•™ìŠµëœ Classification modelì€ Object detectionê³¼ ê°™ì€ ë¶„ì•¼ì—ì„œ backbone networkë¡œ ë§ì´ ì‚¬ìš©ëœë‹¤. Model accuracy ë¿ë§Œì´ ì•„ë‹Œ, memoryë¥¼ ì°¨ì§€í•˜ëŠ” ë¹„ì¤‘ê³¼ inference latencyë„ ì¤‘ìš”í•œ ìš”ì†Œì´ê¸° ë•Œë¬¸ì— ë„¤íŠ¸ì›Œí¬ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ë§Œë“¤ í•„ìš”ì„±ì´ ìˆë‹¤.</li>
</ul>

<blockquote>
  <p>In this paper, we aim to study model efficiency for super large ConvNets that surpass state-of-the-art accuracy. To achieve this goal, we resort to model scaling.</p>
</blockquote>

<ul>
  <li>Modelì˜ íš¨ìœ¨ì„±ì„ ë†’ì´ë ¤ë©´, Model compressionì„ í•˜ê±°ë‚˜ <a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w33/Gholami_SqueezeNext_Hardware-Aware_Neural_CVPR_2018_paper.pdf">SqueezeNets(Gholami et al.,2018)</a>, <a href="https://arxiv.org/abs/1704.04861">MobileNets(Howard et al.,2017)</a>, <a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0642.pdf">ShuffleNets(Zhang et al.,2018)</a>ê³¼ ê°™ì€ handcraft modelì„ ì‚¬ìš©í•˜ê³¤ í•˜ì˜€ë‹¤. <a href="https://arxiv.org/pdf/1807.11626">MnasNet(Tan et al.,2019)</a>ì€ ConvNetì˜ width, depth, kernel type/sizeë¥¼ ì¡°ì ˆí•˜ì—¬ handcraft model ë³´ë‹¤ ë” ì¢‹ì€ íš¨ìœ¨ì„ ë³´ì´ëŠ” mobile-size ëª¨ë¸ì´ë‹¤.</li>
  <li>í•˜ì§€ë§Œ MNasNetì˜ ê¸°ë²•ì€ ë” í° ëª¨ë¸(design spaceê°€ ë„“ì–´ tuningì´ ì–´ë ¤ìš´ ëª¨ë¸)ì— ì ìš©í•˜ê¸° ì–´ë µë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤. ë”°ë¼ì„œ ì €ìë“¤ì€ ë” í° ëª¨ë¸ì—ë„ ì ìš©í•  ìˆ˜ ìˆëŠ” ê¸°ë²•ì— ëŒ€í•´ ì—°êµ¬í•˜ì˜€ë‹¤.</li>
</ul>

<p><br /></p>
<h2 id="compound-model-scaling">Compound Model Scaling</h2>
<hr />
<h3 id="problem-formulation">Problem Formulation</h3>
<ul>
  <li>ConvNet Layer <script type="math/tex">i</script>ì˜ í•¨ìˆ˜ëŠ” <script type="math/tex">\mathrm{Y}_i = \mathcal{F}_i(\mathrm{X}_i)</script>ë¡œ ì •ì˜ëœë‹¤. (<script type="math/tex">\mathrm{Y}_i</script> : output tensor, <script type="math/tex">\mathcal{F}_i</script> : operator, <script type="math/tex">\mathrm{X}_i</script> : input tensor)</li>
  <li>ConvNet <script type="math/tex">\mathcal{N}</script>ì€ layerë“¤ ê°„ì˜ ê²°í•©ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤. (<script type="math/tex">\mathcal{N}=\mathcal{F_k}\odot\ldots\odot\mathcal{F_2}\odot\mathcal{F_1}(\mathrm{X_1})=\bigodot_{j=1 \ldots k}\mathcal{F_j}(\mathrm{X_1})</script>)</li>
  <li>ConvNetì€ ì—¬ëŸ¬ê°œì˜ stageë¡œ ë‚˜ë‰˜ê³ , ê° stageì˜ layerë“¤ì€ ì¼ë°˜ì ìœ¼ë¡œ ë™ì¼í•œ êµ¬ì¡°ë¥¼ ê°€ì§„ë‹¤.</li>
  <li>ê¸°ì¡´ì˜ ë°©ë²•ë“¤ì€ ìµœì  ë ˆì´ì–´ êµ¬ì¡° <script type="math/tex">\mathcal{F_i}</script>ë¥¼ ì°¾ìœ¼ë ¤ í•˜ì˜€ë‹¤. í•˜ì§€ë§Œ <script type="math/tex">\mathcal{F_i}</script>ë¥¼ ê³ ì •í•˜ë©´ design spaceê°€ ì¤„ì–´ë“¤ê¸° ë•Œë¬¸ì— model scalingì´ ì‰¬ì›Œì§„ë‹¤.</li>
  <li>ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  <script type="math/tex">\mathrm{L_i}, \mathrm{C_i}, \mathrm{H_i}, \mathrm{W_i}</script>ë¥¼ ê°ê° layerë§ˆë‹¤ ì¡°ì ˆí•˜ì—¬ ìµœì ì˜ ê°’ì„ ì°¾ëŠ” ê²ƒì€ ì–´ë ¤ìš´ ë¬¸ì œì´ë‹¤. ë”°ë¼ì„œ ëª¨ë“  layerë¥¼ ê°™ì€ ë¹„ìœ¨ë¡œ ì¤„ì´ëŠ” ê²ƒì„ ì œì•½ì¡°ê±´ìœ¼ë¡œ ì •í•œë‹¤.</li>
</ul>

<p align="center"><img src="https://drive.google.com/uc?id=1qfZoJYrAqOol_1bjyton796H7ORx2T4X" width="75%" height="100%" /></p>
<p><br /></p>
<ul>
  <li>Model scalingì„ ìœ„ì˜ ì‹ê³¼ ê°™ì€ <span style="background-color:#BFFF00"> ë©”ëª¨ë¦¬ ê³µê°„ì— ëŒ€í•œ ì œì•½ì¡°ê±´ì´ ì£¼ì–´ì§ˆ ë•Œ, model accuracyë¥¼ ìµœëŒ€í™” í•˜ëŠ” ìµœì í™” ë¬¸ì œ</span>ë¡œ ë°”ê¾¸ì–´ í’€ê³ ì í•œë‹¤.</li>
  <li><script type="math/tex">w,d,r</script>ì€ ê°ê° ë„¤íŠ¸ì›Œí¬ì˜ width, depth, resolutionì— ê³±í•´ì§€ëŠ” coefficientsì´ë©°, <script type="math/tex">\hat{\mathcal{F_i}},\hat{\mathrm{L_i}},\hat{\mathrm{H_i}},\hat{\mathrm{W_i}},\hat{\mathrm{C_i}}</script>ëŠ” ê°ê° baseline ë„¤íŠ¸ì›Œí¬ì˜ predefined parameterì´ë‹¤.</li>
</ul>

<h3 id="scaling-dimensions">Scaling Dimensions</h3>
<blockquote>
  <p>The main difficulty of the problem is that the optimal d, w, r depend on each other and the values change under different resource constrain.</p>
</blockquote>

<ul>
  <li>
    <p>ìœ„ ìµœì í™” ë¬¸ì œì—ì„œ ê°€ì¥ í° ì–´ë ¤ì›€ì€ ê°ê°ì˜ <script type="math/tex">w,d,r</script>ì´ ì˜ì¡´ì ì¸ ê°’ì´ë©° ë§¤ë²ˆ ë‹¤ë¥¸ ë©”ëª¨ë¦¬ ì¡°ê±´ì— ë”°ë¼ ê°’ë“¤ì´ ë³€í•œë‹¤ëŠ” ê²ƒì´ë‹¤. ì´ëŸ° ì´ìœ ë¡œ ì¸í•´ ê¸°ì¡´ì˜ ë°©ë²•ë“¤ì€ í•˜ë‚˜ì˜ ê°’ë§Œ ì¡°ì ˆí•˜ëŠ” ê¸°ë²•ì„ ì‚¬ìš©í–ˆë‹¤. 3ê°€ì§€ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ì˜í–¥ì„ ë¯¸ì¹œë‹¤.</p>

    <ol>
      <li>
        <ul>
          <li><strong>Depth</strong> : ë„¤íŠ¸ì›Œí¬ì˜ depthê°€ ì»¤ì§ˆ ìˆ˜ë¡, ë” ë³µì¡í•œ featureë¥¼ capture í•  ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ vanishing graident ë¬¸ì œê°€ ë°œìƒí•œë‹¤.</li>
        </ul>
      </li>
      <li>
        <ul>
          <li><strong>Width</strong> : ë„¤íŠ¸ì›Œí¬ì˜ widthê°€ ë„“ì–´ì§ˆ ìˆ˜ë¡, ë” ë¯¸ì„¸í•œ featureë¥¼ capture í•  ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ depthê°€ ì¶©ë¶„íˆ ê¹Šì§€ ì•Šë‹¤ë©´, ì¶”ìƒì •ë³´(high-level features)ë¥¼ íšë“í•˜ê¸° ì–´ë µë‹¤.</li>
        </ul>
      </li>
      <li>
        <ul>
          <li><strong>Resolution</strong> : ì…ë ¥ ì˜ìƒì˜ í•´ìƒë„ê°€ í´ ìˆ˜ë¡, ë” ì •ë°€í•œ íŒ¨í„´ì„ capture í•  ìˆ˜ ìˆë‹¤.</li>
        </ul>
      </li>
    </ol>
  </li>
</ul>

<p align="center"><img src="https://drive.google.com/uc?id=1FnnOo3yfR0FnXrDOA2w3QZxRTjd3BWd9" width="100%" height="100%" /></p>

<ul>
  <li>Figure3ì—ì„œ ì²˜ëŸ¼ ê°ê° íŒŒë¼ë¯¸í„°ì˜ ê°’ì„ ì˜¬ë¦´ ìˆ˜ë¡ ì„±ëŠ¥ì´ ì˜¬ë¼ê°„ë‹¤. í•˜ì§€ë§Œ ì–´ëŠì •ë„ ì˜¬ë¼ê°€ë©´ ì„±ëŠ¥ì´ ìˆ˜ë ´í•˜ê²Œ ëœë‹¤(=accuracy gainì´ ì ì–´ì§„ë‹¤).</li>
</ul>

<h3 id="compound-scaling">Compound Scaling</h3>

<p align="center"><img src="https://drive.google.com/uc?id=1Q86GB1Q99Y-oNMjkz2g6bWTtI9qkTC-I" width="70%" height="100%" /></p>

<blockquote>
  <p>In order to pursue better accuracy and efficiency, it is critical to balance all dimensions of network width, depth, and resolution during ConvNet scaling.</p>
</blockquote>

<ul>
  <li>ì§ê´€ì ìœ¼ë¡œ, ì…ë ¥ ì´ë¯¸ì§€ì˜ í•´ìƒë„ê°€ ì»¤ì§€ë©´ ë” ë§ì€ í”½ì…€ì •ë³´ë¥¼ ë‹´ê¸° ìœ„í•´ì„œ ëª¨ë¸ì˜ depth, widthë„ ì»¤ì ¸ì•¼ í•œë‹¤. ì´ì— ëŒ€í•œ ì‹¤í—˜ ê²°ê³¼ëŠ” Figure4ì™€ ê°™ë‹¤. depth, resolutionì„ ê°ê° ì¡°ì ˆí•˜ëŠ” ê²ƒ ë³´ë‹¤, ë‘ê°€ì§€ ë‹¤ ì¡°ì ˆí•˜ëŠ” ê²ƒì´ ì„±ëŠ¥ì´ ë” ì¢‹ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.</li>
  <li><span style="background-color:#BFFF00">ë”°ë¼ì„œ ì‹¤í—˜ì„ í†µí•´ ì•Œ ìˆ˜ ìˆëŠ” ì‚¬ì‹¤ì€ depth, width, resolutionì„ ì ì ˆí•˜ê²Œ ì¡°í•©í•˜ì—¬ scalingí•˜ë©´ íš¨ìœ¨ì ì´ë©° ë†’ì€ ì„±ëŠ¥ì„ ì–»ì„ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤.</span></li>
</ul>

<p align="center"><img src="https://drive.google.com/uc?id=1bASUuxuC7pzMLZbFRLTYjZeLuhRn1dGn" width="70%" height="100%" /></p>

<ul>
  <li>ì €ìë“¤ì€ 3ê°€ì§€ íŒŒë¼ë¯¸í„°ë¥¼ ì›ì¹™ì— ì…ê°í•˜ì—¬ ë³€ê²½í•  ìˆ˜ ìˆëŠ”, <strong>Compound scaling method</strong>ë¥¼ ì œì•ˆí•œë‹¤.</li>
  <li><script type="math/tex">\phi</script>ëŠ” width, depth, resolutionì— uniformlyí•˜ê²Œ ê³±í•´ì§€ëŠ” ê³„ìˆ˜ì´ë©°, <script type="math/tex">\alpha, \beta, \gamma</script>ëŠ” grid searchë¥¼ í†µí•´ ì–»ì€ ìƒìˆ˜ë“¤ì´ë‹¤.</li>
  <li>ì§ê´€ì ìœ¼ë¡œ, <script type="math/tex">\phi</script>ëŠ” ëª¨ë¸ í™•ì¥ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë¦¬ì†ŒìŠ¤ë¥¼ ì œì–´í•˜ëŠ” ë³€ìˆ˜ì´ë©°, ê°€ìš©ë©”ëª¨ë¦¬ì˜ ìš©ëŸ‰ì´ í´ ìˆ˜ë¡ ê°’ì„ ì˜¬ë¦´ ìˆ˜ ìˆë‹¤. <script type="math/tex">\alpha, \beta, \gamma</script>ëŠ” ë„¤íŠ¸ì›Œí¬ì˜ width, depth, resolutionì˜ í• ë‹¹ ë¹„ì¤‘ì„ ë‚˜íƒ€ë‚´ëŠ” ê°’ì´ë‹¤.</li>
  <li>Convolution ì—°ì‚°ì´ ëŒ€ë¶€ë¶„ì´ê¸° ë•Œë¬¸ì—, FLOPS ë˜í•œ convolutionì— ë¹„ë¡€í•œë‹¤. ê·¸ë¦¬ê³  convolution ì—°ì‚°ì€ <script type="math/tex">d, w^2, r^2</script>ì— ë¹„ë¡€í•œë‹¤. ë”°ë¼ì„œ FLOPSëŠ” <script type="math/tex">(\alpha\cdot\beta^2\cdot\gamma^2)^\phi</script>ì— ë¹„ë¡€í•˜ê²Œ ëœë‹¤. ì €ìë“¤ì€ (<script type="math/tex">\alpha\cdot\beta^2\cdot\gamma^2</script>)ë¥¼ 2ì— ê·¼ì ‘í•œ ê°’ì´ ë‚˜ì˜¤ë„ë¡ ì„¤ì •í•˜ì—¬, ì´ FLOPSê°€ <script type="math/tex">2^\phi</script>ê°€ ë˜ë„ë¡ í•˜ì˜€ë‹¤.</li>
</ul>

<p><br /></p>
<h2 id="efficientnet-architecture">EfficientNet Architecture</h2>
<hr />

<p align="center"><img src="https://drive.google.com/uc?id=1JW0aOK1Y1T7J-Oe863kda2k9eoPJF-Fr" width="70%" height="100%" /></p>

<blockquote>
  <p>Since model scaling does not change layer operators <script type="math/tex">\hat{\mathcal{F_i}}</script> in baseline network, having a good baseline network is also critical. We will evaluate our scaling method using existing ConvNets, but in order to better demonstrate the effectiveness of our scaling method, we have also developed a new mobile-size baseline, called EfficientNet.</p>
</blockquote>

<ul>
  <li>ì•„ë¬´ë¦¬ model scalingì„ íš¨ê³¼ì ìœ¼ë¡œ í•˜ë”ë¼ë„, baseline networkê°€ ì¢‹ì§€ ì•Šìœ¼ë©´ ì„±ëŠ¥ í–¥ìƒì— í•œê³„ê°€ ìˆë‹¤. ë”°ë¼ì„œ ì €ìë“¤ì€ EfficientNet-B0ë¼ëŠ” ìƒˆë¡œìš´ mobile-size networkë¥¼ ì„¤ê³„í•˜ì˜€ë‹¤(Table1).</li>
</ul>

<h3 id="mbconv">MBConv</h3>
<ul>
  <li>MBConvëŠ” MobileNet V2ì—ì„œ ì œì•ˆëœ blockì´ë©°, efficientNetì€ MBConvë¥¼ ì‚¬ìš©í•œ MNasNetê³¼ ë¹„ìŠ·í•œ êµ¬ì¡°ì˜ ëª¨ë¸ì„ baseline(EfficientNet B0)ìœ¼ë¡œ êµ¬ì¶•í•˜ì˜€ë‹¤.</li>
  <li>
    <p>MBConvì˜ íŠ¹ì§•ì€ í¬ê²Œ 3ê°€ì§€ë¡œ êµ¬ë¶„í•  ìˆ˜ ìˆë‹¤. ìì„¸í•œ ì„¤ëª…ì€ ë§í¬ ì°¸ì¡°<a href="https://towardsdatascience.com/mobilenetv2-inverted-residuals-and-linear-bottlenecks-8a4362f4ffd5">(MobileNetV2: Inverted Residuals and Linear Bottlenecks)</a></p>

    <ol>
      <li>
        <ul>
          <li>Depthwise convolution + Pointwise convolution : ê¸°ë³¸ convolution ì—°ì‚°ì„ 2ë‹¨ê³„ë¡œ ë¶„ë¦¬í•˜ì—¬ ì—°ì‚°ëŸ‰ì„ ì¤„ì¸ë‹¤.</li>
        </ul>
      </li>
      <li>
        <ul>
          <li>Inverse residuals : Residual ë¸”ë¡ì€ channelì„ ì••ì¶•í•˜ëŠ” ë ˆì´ì–´ì™€ í™•ì¥í•˜ëŠ” ë ˆì´ì–´ë¡œ êµ¬ì„±ëœë‹¤. ê¸°ì¡´ residual ë°©ì‹ì—ì„œëŠ” ì±„ë„ì´ í™•ì¥ëœ ë ˆì´ì–´ ë¼ë¦¬ ì—°ê²°ì´ ë˜ëŠ” ë°˜ë©´, MBconvëŠ” ì ì€ ì±„ë„ë¼ë¦¬ skip connectionì„ í˜•ì„±í•˜ê²Œ ë˜ì–´ ì—°ì‚°ëŸ‰ì´ ê°ì†Œëœë‹¤.</li>
        </ul>
      </li>
      <li>
        <ul>
          <li>Linear bottleneck : Relu í™œì„±í™” í•¨ìˆ˜ë¡œ ì¸í•œ ì •ë³´ ì†ì‹¤ì„ ë°©ì§€í•˜ê¸° ìœ„í•´, ê° ë¸”ë¡ì˜ ë§ˆì§€ë§‰ ê³„ì¸µì—ì„œ ì„ í˜• í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤.</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>EfficientNet-B0ëŠ” latencyë³´ë‹¤ FLOPSë¥¼ ëª©í‘œë¡œ ìµœì í™”í•˜ì˜€ëŠ”ë°, ì´ëŠ” íŠ¹ì • í•˜ë“œì›¨ì–´ë¥¼ ëª©í‘œë¡œ í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆê¸° ë•Œë¬¸ì´ë‹¤.</li>
  <li>
    <p>EfficientNet-B0ë¥¼ baselineìœ¼ë¡œ compound scalingì„ ë‹¤ìŒê³¼ ê°™ì´ ì ìš©í•œë‹¤.</p>

    <ol>
      <li>
        <ul>
          <li>STEP1 : <script type="math/tex">\phi=1</script>ë¡œ ê³ ì •í›„, ë©”ëª¨ë¦¬ ìì›ì´ 2ë°°ë¼ê³  ê°€ì •í•  ë•Œ ìµœì ì˜ <script type="math/tex">\alpha, \beta, \gamma</script> ê°’ì„ ìˆ˜ì‹ 2, 3ì„ ê¸°ë°˜ìœ¼ë¡œ small grid searchë¡œ ì°¾ëŠ”ë‹¤.</li>
        </ul>
      </li>
      <li>
        <ul>
          <li>STEP2 : <script type="math/tex">\alpha, \beta, \gamma</script>ì„ ê³ ì •í•˜ê³ , <script type="math/tex">\phi=1</script>ì„ ë³€í™”ì‹œì¼œ scale upí•œë‹¤.</li>
        </ul>
      </li>
    </ol>
  </li>
</ul>

<p><br /></p>
<h2 id="experiments">Experiments</h2>
<hr />
<p align="center"><img src="https://drive.google.com/uc?id=1Own0qoGYxXg5hc8oQpFGhLpZhiSTNa1o" width="70%" height="100%" /></p>
<ul>
  <li>EfficientNetì´ ë‹¤ë¥¸ ë„¤íŠ¸ì›Œí¬ì— ë¹„í•´ í›¨ì”¬ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.</li>
  <li>íŠ¹íˆ ì—°ì‚°ëŸ‰ì„ ì¤„ì˜€ìŒì—ë„ ë¶ˆêµ¬í•˜ê³ , accuracyê°€ ë” ì˜¬ë¼ê°„ ê²ƒì€ compound scalingì´ ë§¤ìš° íš¨ê³¼ì ì´ë‹¤ë¼ëŠ” ê²ƒì„ ë³´ì—¬ì¤€ë‹¤.</li>
  <li>ë©”ëª¨ë¦¬ë¥¼ ì¤„ì´ëŠ” ê²ƒì„ íƒ€ê²Ÿìœ¼ë¡œ í•˜ì˜€ì§€ë§Œ, ëª¨ë¸ì´ ì‘ì•„ì§€ë©´ì„œ ì—°ì‚°ëŸ‰ì´ ì¤„ì–´ ê²°ê³¼ì ìœ¼ë¡œ inference latencyë„ ì‘ì•„ì§€ëŠ” ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ì—ˆë‹¤(ë…¼ë¬¸ Table 4 ì°¸ê³ ).</li>
</ul>

<p align="center"><img src="https://drive.google.com/uc?id=1vYlpLb9-x1jlx7ryp8IEvzY-zKuOE9Fm" width="100%" height="100%" /></p>
<ul>
  <li>CAM(Class activation map)ì„ ë³´ì•„ë„ compound scalingì„ í•˜ì˜€ì„ ë•Œ íƒ€ê²Ÿ ê°ì²´ì— ì´ˆì ì´ ë” ë§ëŠ” ê²ƒì„ ë³´ì—¬ì¤€ë‹¤.</li>
</ul>

<p><br /></p>
<h2 id="conclusion">Conclusion</h2>
<hr />
<ul>
  <li>í•œì¤„ ìš”ì•½ : Model scalingì—ì„œ width, depth, resolutionì„ <strong>ì ì ˆí•œ ë¹„ìœ¨</strong>ë¡œ ì¦ê°€í•´ì•¼ accuracyì™€ efficiencyì—ì„œ ì†ì‹¤ ì—†ì´ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤.</li>
</ul>
:ET