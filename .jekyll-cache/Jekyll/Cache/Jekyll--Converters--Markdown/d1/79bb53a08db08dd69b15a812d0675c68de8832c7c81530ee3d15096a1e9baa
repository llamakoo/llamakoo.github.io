I"|&<blockquote>
  <p>Title : EfficientNet, Rethinking Model Scaling for Convolutional Neural Network</p>
</blockquote>

<blockquote>
  <p>Paper link : <a href="https://arxiv.org/pdf/1905.11946.pdf">https://arxiv.org/pdf/1905.11946.pdf</a></p>
</blockquote>

<blockquote>
  <p>Publised year : 23 Nov 2019</p>
</blockquote>

<blockquote>
  <p>keywords : Model Scaling, Classification</p>
</blockquote>

<hr />

<blockquote>
  <p>In this paper, we systematically study model scaling and identify that carefully <strong>balancing network</strong> depth, width, and resolution can lead to better performance. Based on this observation, we propose <strong>a new scaling method that uniformly scales all dimensions of depth/width/resolution</strong> using a simple yet highly effective <strong>compound coefficient</strong>.</p>
</blockquote>

<p><img src="https://drive.google.com/uc?id=1tgBVYooCbdxLcHi6eByFyXQfEGMVPKY7" alt="Model Scaling" width="100%" height="100%" /></p>

<p><br /></p>
<h2 id="introduction">Introduction</h2>
<hr />

<blockquote>
  <p>In previous work, it is common to scale only one of the three dimensions – depth, width, and image size. Though it is possible to scale two or three dimensions arbitrarily, arbitrary scaling requires tedious manual tuning and still often yields sub-optimal accuracy and efficiency. …  In particular, we investigate the central question: is there a principled method to scale up ConvNets that can achieve better accuracy and efficiency?</p>
</blockquote>

<ul>
  <li>기존 연구에서는 성능을 올리기 위해 depth, width, image size 중 하나만 조절하여 수동으로 튜닝을 하였다. 하지만 이러한 방법으로는 최적값을 찾기 어렵다. 저자들은 <span style="background-color:#BFFF00">“3가지 요소를 적절하게 조절하여 성능을 최적화하는 방법이 없을까?”</span> 라는 질문에서 부터 시작하여 논지를 전개한다.</li>
</ul>

<blockquote>
  <p>Unlike conventional practice that arbitrary scales these factors, our method uniformly scales network width, depth, and resolution with a set of fixed scaling coefficients.</p>
</blockquote>

<ul>
  <li>본 논문에서는 그런 방법의 일환으로 임의의 값이 아닌, 고정된 scaling coefficients로 동일하게 적용할 수 있는 <strong>“Compound scaling method”</strong>를 소개한다.</li>
  <li>ImageNet competition에서 <a href="https://arxiv.org/abs/1811.06965">GPipe(Huang et al.,2018)</a>는 84.3%의 가장 높은 accuracy를 보였지만, 파라미터가 556M으로 많은 메모리를 차지하게 된다는 단점이 존재한다.</li>
  <li>ImageNet으로 학습된 Classification model은 Object detection과 같은 분야에서 backbone network로 많이 사용된다. Model accuracy 뿐만이 아닌, memory를 차지하는 비중과 inference latency도 중요한 요소이기 때문에 네트워크를 효율적으로 만들 필요성이 있다.</li>
</ul>

<blockquote>
  <p>In this paper, we aim to study model efficiency for super large ConvNets that surpass state-of-the-art accuracy. To achieve this goal, we resort to model scaling.</p>
</blockquote>

<ul>
  <li>Model의 효율성을 높이려면, Model compression을 하거나 <a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w33/Gholami_SqueezeNext_Hardware-Aware_Neural_CVPR_2018_paper.pdf">SqueezeNets(Gholami et al.,2018)</a>, <a href="https://arxiv.org/abs/1704.04861">MobileNets(Howard et al.,2017)</a>, <a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0642.pdf">ShuffleNets(Zhang et al.,2018)</a>과 같은 handcraft model을 사용하곤 하였다. <a href="https://arxiv.org/pdf/1807.11626">MnasNet(Tan et al.,2019)</a>은 ConvNet의 width, depth, kernel type/size를 조절하여 handcraft model 보다 더 좋은 효율을 보이는 mobile-size 모델이다.</li>
  <li>하지만 MNasNet의 기법은 더 큰 모델(design space가 넓어 tuning이 어려운 모델)에 적용하기 어렵다는 단점이 있다. 따라서 저자들은 더 큰 모델에도 적용할 수 있는 기법에 대해 연구하였다.</li>
</ul>

<p><br /></p>
<h2 id="compound-model-scaling">Compound Model Scaling</h2>
<hr />
<h3 id="problem-formulation">Problem Formulation</h3>
<ul>
  <li>ConvNet Layer <script type="math/tex">i</script>의 함수는 <script type="math/tex">\mathrm{Y}_i = \mathcal{F}_i(\mathrm{X}_i)</script>로 정의된다. (<script type="math/tex">\mathrm{Y}_i</script> : output tensor, <script type="math/tex">\mathcal{F}_i</script> : operator, <script type="math/tex">\mathrm{X}_i</script> : input tensor)</li>
  <li>ConvNet <script type="math/tex">\mathcal{N}</script>은 layer들 간의 결합으로 표현할 수 있다. (<script type="math/tex">\mathcal{N}=\mathcal{F_k}\odot\ldots\odot\mathcal{F_2}\odot\mathcal{F_1}(\mathrm{X_1})=\bigodot_{j=1 \ldots k}\mathcal{F_j}(\mathrm{X_1})</script>)</li>
  <li>ConvNet은 여러개의 stage로 나뉘고, 각 stage의 layer들은 일반적으로 동일한 구조를 가진다.</li>
  <li>기존의 방법들은 최적 레이어 구조 <script type="math/tex">\mathcal{F_i}</script>를 찾으려 하였다. 하지만 <script type="math/tex">\mathcal{F_i}</script>를 고정하면 design space가 줄어들기 때문에 model scaling이 쉬워진다.</li>
  <li>그럼에도 불구하고 <script type="math/tex">\mathrm{L_i}, \mathrm{C_i}, \mathrm{H_i}, \mathrm{W_i}</script>를 각각 layer마다 조절하여 최적의 값을 찾는 것은 어려운 문제이다. 따라서 모든 layer를 같은 비율로 줄이는 것을 제약조건으로 정한다.</li>
</ul>

<p align="center"><img src="https://drive.google.com/uc?id=1qfZoJYrAqOol_1bjyton796H7ORx2T4X" width="75%" height="100%" /></p>
<p><br /></p>
<ul>
  <li>Model scaling을 위의 식과 같은 <span style="background-color:#BFFF00"> 메모리 공간에 대한 제약조건이 주어질 때, model accuracy를 최대화 하는 최적화 문제</span>로 바꾸어 풀고자 한다.</li>
  <li><script type="math/tex">w,d,r</script>은 각각 네트워크의 width, depth, resolution에 곱해지는 coefficients이며, <script type="math/tex">\hat{\mathcal{F_i}},\hat{\mathrm{L_i}},\hat{\mathrm{H_i}},\hat{\mathrm{W_i}},\hat{\mathrm{C_i}}</script>는 각각 baseline 네트워크의 predefined parameter이다.</li>
</ul>

<h3 id="scaling-dimensions">Scaling Dimensions</h3>
<blockquote>
  <p>The main difficulty of the problem is that the optimal d, w, r depend on each other and the values change under different resource constrain.</p>
</blockquote>

<ul>
  <li>
    <p>위 최적화 문제에서 가장 큰 어려움은 각각의 <script type="math/tex">w,d,r</script>이 의존적인 값이며 매번 다른 메모리 조건에 따라 값들이 변한다는 것이다. 이런 이유로 인해 기존의 방법들은 하나의 값만 조절하는 기법을 사용했다. 3가지의 파라미터를 조정하면 다음과 같은 영향을 미친다.</p>

    <ol>
      <li>
        <ul>
          <li><strong>Depth</strong> : 네트워크의 depth가 커질 수록, 더 복잡한 feature를 capture 할 수 있다. 하지만 vanishing graident 문제가 발생한다.</li>
        </ul>
      </li>
      <li>
        <ul>
          <li><strong>Width</strong> : 네트워크의 width가 넓어질 수록, 더 미세한 feature를 capture 할 수 있다. 하지만 depth가 충분히 깊지 않다면, 추상정보(high-level features)를 획득하기 어렵다.</li>
        </ul>
      </li>
      <li>
        <ul>
          <li><strong>Resolution</strong> : 입력 영상의 해상도가 클 수록, 더 정밀한 패턴을 capture 할 수 있다.</li>
        </ul>
      </li>
    </ol>
  </li>
</ul>

<p align="center"><img src="https://drive.google.com/uc?id=1FnnOo3yfR0FnXrDOA2w3QZxRTjd3BWd9" width="100%" height="100%" /></p>

<ul>
  <li>Figure3에서 처럼 각각 파라미터의 값을 올릴 수록 성능이 올라간다. 하지만 어느정도 올라가면 성능이 수렴하게 된다(=accuracy gain이 적어진다).</li>
</ul>

<h3 id="compound-scaling">Compound Scaling</h3>

<p align="center"><img src="https://drive.google.com/uc?id=1Q86GB1Q99Y-oNMjkz2g6bWTtI9qkTC-I" width="70%" height="100%" /></p>

<blockquote>
  <p>In order to pursue better accuracy and efficiency, it is critical to balance all dimensions of network width, depth, and resolution during ConvNet scaling.</p>
</blockquote>

<ul>
  <li>직관적으로, 입력 이미지의 해상도가 커지면 더 많은 픽셀정보를 담기 위해서 모델의 depth, width도 커져야 한다. 이에 대한 실험 결과는 Figure4와 같다. depth, resolution을 각각 조절하는 것 보다, 둘 다 조절하는 것이 성능이 더 좋다는 것을 알 수 있다.</li>
</ul>

<p align="center"><img src="https://drive.google.com/uc?id=1bASUuxuC7pzMLZbFRLTYjZeLuhRn1dGn" width="70%" height="100%" /></p>

<ul>
  <li>저자들은 3가지 파라미터를 원칙에 입각하여 변경할 수 있는, <strong>Compound scaling method</strong>를 제안한다.</li>
  <li><script type="math/tex">\phi</script>는 width, depth, resolution에 uniformly하게 곱해지는 계수이며, <script type="math/tex">\alpha, \beta, \gamma</script>는 samll grid search를 통해 얻은 상수들이다.</li>
  <li>직관적으로, <script type="math/tex">\phi</script>는 모델 확장에 사용할 수 있는 리소스를 제어하는 변수이며, 가용메모리의 용량이 클 수록 값을 올릴 수 있다. <script type="math/tex">\alpha, \beta, \gamma</script>는 네트워크의 width, depth, resolution의 할당 비중을 나타내는 값이다.</li>
</ul>

<h2 id="efficientnet-architecture">EfficientNet Architecture</h2>
<hr />

<h2 id="experiments">Experiments</h2>
<hr />

<h2 id="conclusion">Conclusion</h2>
<hr />
:ET