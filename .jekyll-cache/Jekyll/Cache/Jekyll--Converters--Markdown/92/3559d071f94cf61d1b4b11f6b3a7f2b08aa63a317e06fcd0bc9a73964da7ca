I"Œ#<blockquote>
  <p>Title : EfficientNet, Rethinking Model Scaling for Convolutional Neural Network</p>
</blockquote>

<blockquote>
  <p>Paper link : <a href="https://arxiv.org/pdf/1905.11946.pdf">https://arxiv.org/pdf/1905.11946.pdf</a></p>
</blockquote>

<blockquote>
  <p>Publised year : 23 Nov 2019</p>
</blockquote>

<blockquote>
  <p>keywords : Model Scaling, Classification</p>
</blockquote>

<hr />

<blockquote>
  <p>In this paper, we systematically study model scaling and identify that carefully <strong>balancing network</strong> depth, width, and resolution can lead to better performance. Based on this observation, we propose <strong>a new scaling method that uniformly scales all dimensions of depth/width/resolution</strong> using a simple yet highly effective <strong>compound coefficient</strong>.</p>
</blockquote>

<p><img src="https://drive.google.com/uc?id=1tgBVYooCbdxLcHi6eByFyXQfEGMVPKY7" alt="Model Scaling" width="100%" height="100%" /></p>

<p><br /></p>
<h2 id="introduction">Introduction</h2>
<hr />

<blockquote>
  <p>In previous work, it is common to scale only one of the three dimensions â€“ depth, width, and image size. Though it is possible to scale two or three dimensions arbitrarily, arbitrary scaling requires tedious manual tuning and still often yields sub-optimal accuracy and efficiency. â€¦  In particular, we investigate the central question: is there a principled method to scale up ConvNets that can achieve better accuracy and efficiency?</p>
</blockquote>

<ul>
  <li>ê¸°ì¡´ ì—°êµ¬ì—ì„œëŠ” ì„±ëŠ¥ì„ ì˜¬ë¦¬ê¸° ìœ„í•´ depth, width, image size ì¤‘ í•˜ë‚˜ë§Œ ì¡°ì ˆí•˜ì—¬ ìˆ˜ë™ìœ¼ë¡œ íŠœë‹ì„ í•˜ì˜€ë‹¤. í•˜ì§€ë§Œ ì´ëŸ¬í•œ ë°©ë²•ìœ¼ë¡œëŠ” ìµœì ê°’ì„ ì°¾ê¸° ì–´ë µë‹¤. ì €ìë“¤ì€ <span style="background-color:#BFFF00">â€œ3ê°€ì§€ ìš”ì†Œë¥¼ ì ì ˆí•˜ê²Œ ì¡°ì ˆí•˜ì—¬ ì„±ëŠ¥ì„ ìµœì í™”í•˜ëŠ” ë°©ë²•ì´ ì—†ì„ê¹Œ?â€</span> ë¼ëŠ” ì§ˆë¬¸ì—ì„œ ë¶€í„° ì‹œì‘í•˜ì—¬ ë…¼ì§€ë¥¼ ì „ê°œí•œë‹¤.</li>
</ul>

<blockquote>
  <p>Unlike conventional practice that arbitrary scales these factors, our method uniformly scales network width, depth, and resolution with a set of fixed scaling coefficients.</p>
</blockquote>

<ul>
  <li>ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ê·¸ëŸ° ë°©ë²•ì˜ ì¼í™˜ìœ¼ë¡œ ì„ì˜ì˜ ê°’ì´ ì•„ë‹Œ, ê³ ì •ëœ scaling coefficientsë¡œ ë™ì¼í•˜ê²Œ ì ìš©í•  ìˆ˜ ìˆëŠ” <strong>â€œCompound scaling methodâ€</strong>ë¥¼ ì†Œê°œí•œë‹¤.</li>
  <li>ImageNet competitionì—ì„œ <a href="https://arxiv.org/abs/1811.06965">GPipe(Huang et al.,2018)</a>ëŠ” 84.3%ì˜ ê°€ì¥ ë†’ì€ accuracyë¥¼ ë³´ì˜€ì§€ë§Œ, íŒŒë¼ë¯¸í„°ê°€ 556Mìœ¼ë¡œ ë§ì€ ë©”ëª¨ë¦¬ë¥¼ ì°¨ì§€í•˜ê²Œ ëœë‹¤ëŠ” ë‹¨ì ì´ ì¡´ì¬í•œë‹¤.</li>
  <li>ImageNetìœ¼ë¡œ í•™ìŠµëœ Classification modelì€ Object detectionê³¼ ê°™ì€ ë¶„ì•¼ì—ì„œ backbone networkë¡œ ë§ì´ ì‚¬ìš©ëœë‹¤. Model accuracy ë¿ë§Œì´ ì•„ë‹Œ, memoryë¥¼ ì°¨ì§€í•˜ëŠ” ë¹„ì¤‘ê³¼ inference latencyë„ ì¤‘ìš”í•œ ìš”ì†Œì´ê¸° ë•Œë¬¸ì— ë„¤íŠ¸ì›Œí¬ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ë§Œë“¤ í•„ìš”ì„±ì´ ìˆë‹¤.</li>
</ul>

<blockquote>
  <p>In this paper, we aim to study model efficiency for super large ConvNets that surpass state-of-the-art accuracy. To achieve this goal, we resort to model scaling.</p>
</blockquote>

<ul>
  <li>Modelì˜ íš¨ìœ¨ì„±ì„ ë†’ì´ë ¤ë©´, Model compressionì„ í•˜ê±°ë‚˜ <a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w33/Gholami_SqueezeNext_Hardware-Aware_Neural_CVPR_2018_paper.pdf">SqueezeNets(Gholami et al.,2018)</a>, <a href="https://arxiv.org/abs/1704.04861">MobileNets(Howard et al.,2017)</a>, <a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0642.pdf">ShuffleNets(Zhang et al.,2018)</a>ê³¼ ê°™ì€ handcraft modelì„ ì‚¬ìš©í•˜ê³¤ í•˜ì˜€ë‹¤. <a href="https://arxiv.org/pdf/1807.11626">MnasNet(Tan et al.,2019)</a>ì€ ConvNetì˜ width, depth, kernel type/sizeë¥¼ ì¡°ì ˆí•˜ì—¬ handcraft model ë³´ë‹¤ ë” ì¢‹ì€ íš¨ìœ¨ì„ ë³´ì´ëŠ” mobile-size ëª¨ë¸ì´ë‹¤.</li>
  <li>í•˜ì§€ë§Œ MNasNetì˜ ê¸°ë²•ì€ ë” í° ëª¨ë¸(design spaceê°€ ë„“ì–´ tuningì´ ì–´ë ¤ìš´ ëª¨ë¸)ì— ì ìš©í•˜ê¸° ì–´ë µë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤. ë”°ë¼ì„œ ì €ìë“¤ì€ ë” í° ëª¨ë¸ì—ë„ ì ìš©í•  ìˆ˜ ìˆëŠ” ê¸°ë²•ì— ëŒ€í•´ ì—°êµ¬í•˜ì˜€ë‹¤.</li>
</ul>

<p><br /></p>
<h2 id="compound-model-scaling">Compound Model Scaling</h2>
<hr />
<h3 id="problem-formulation">Problem Formulation</h3>
<ul>
  <li>ConvNet Layer <script type="math/tex">i</script>ì˜ í•¨ìˆ˜ëŠ” <script type="math/tex">\mathrm{Y}_i = \mathcal{F}_i(\mathrm{X}_i)</script>ë¡œ ì •ì˜ëœë‹¤. (<script type="math/tex">\mathrm{Y}_i</script> : output tensor, <script type="math/tex">\mathcal{F}_i</script> : operator, <script type="math/tex">\mathrm{X}_i</script> : input tensor)</li>
  <li>ConvNet <script type="math/tex">\mathcal{N}</script>ì€ layerë“¤ ê°„ì˜ ê²°í•©ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤. (<script type="math/tex">\mathcal{N}=\mathcal{F_k}\odot\ldots\odot\mathcal{F_2}\odot\mathcal{F_1}(\mathrm{X_1})=\bigodot_{j=1 \ldots k}\mathcal{F_j}(\mathrm{X_1})</script>)</li>
  <li>ConvNetì€ ì—¬ëŸ¬ê°œì˜ stageë¡œ ë‚˜ë‰˜ê³ , ê° stageì˜ layerë“¤ì€ ì¼ë°˜ì ìœ¼ë¡œ ë™ì¼í•œ êµ¬ì¡°ë¥¼ ê°€ì§„ë‹¤.</li>
  <li>ê¸°ì¡´ì˜ ë°©ë²•ë“¤ì€ ìµœì  ë ˆì´ì–´ êµ¬ì¡° <script type="math/tex">\mathcal{F_i}</script>ë¥¼ ì°¾ìœ¼ë ¤ í•˜ì˜€ë‹¤. í•˜ì§€ë§Œ <script type="math/tex">\mathcal{F_i}</script>ë¥¼ ê³ ì •í•˜ë©´ design spaceê°€ ì¤„ì–´ë“¤ê¸° ë•Œë¬¸ì— model scalingì´ ì‰¬ì›Œì§„ë‹¤.</li>
  <li>ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  <script type="math/tex">\mathrm{L_i}, \mathrm{C_i}, \mathrm{H_i}, \mathrm{W_i}</script>ë¥¼ ê°ê° layerë§ˆë‹¤ ì¡°ì ˆí•˜ì—¬ ìµœì ì˜ ê°’ì„ ì°¾ëŠ” ê²ƒì€ ì–´ë ¤ìš´ ë¬¸ì œì´ë‹¤. ë”°ë¼ì„œ ëª¨ë“  layerë¥¼ ê°™ì€ ë¹„ìœ¨ë¡œ ì¤„ì´ëŠ” ê²ƒì„ ì œì•½ì¡°ê±´ìœ¼ë¡œ ì •í•œë‹¤.</li>
</ul>

<p align="center"><img src="https://drive.google.com/uc?id=1qfZoJYrAqOol_1bjyton796H7ORx2T4X" width="75%" height="100%" /></p>
<p><br /></p>
<ul>
  <li>Model scalingì„ ìœ„ì˜ ì‹ê³¼ ê°™ì€ <span style="background-color:#BFFF00"> ë©”ëª¨ë¦¬ ê³µê°„ì— ëŒ€í•œ ì œì•½ì¡°ê±´ì´ ì£¼ì–´ì§ˆ ë•Œ, model accuracyë¥¼ ìµœëŒ€í™” í•˜ëŠ” ìµœì í™” ë¬¸ì œ</span>ë¡œ ë°”ê¾¸ì–´ í’€ê³ ì í•œë‹¤.</li>
  <li><script type="math/tex">w,d,r</script>ì€ ê°ê° ë„¤íŠ¸ì›Œí¬ì˜ width, depth, resolutionì— ê³±í•´ì§€ëŠ” coefficientsì´ë©°, <script type="math/tex">\hat{\mathcal{F_i}},\hat{\mathrm{L_i}},\hat{\mathrm{H_i}},\hat{\mathrm{W_i}},\hat{\mathrm{C_i}}</script>ëŠ” ê°ê° baseline ë„¤íŠ¸ì›Œí¬ì˜ predefined parameterì´ë‹¤.</li>
</ul>

<h3 id="scaling-dimensions">Scaling Dimensions</h3>
<blockquote>
  <p>The main difficulty of the problem is that the optimal d, w, r depend on each other and the values change under different resource constrain.</p>
</blockquote>

<ul>
  <li>
    <p>ìœ„ ìµœì í™” ë¬¸ì œì—ì„œ ê°€ì¥ í° ì–´ë ¤ì›€ì€ ê°ê°ì˜ <script type="math/tex">w,d,r</script>ì´ ì˜ì¡´ì ì¸ ê°’ì´ë©° ë§¤ë²ˆ ë‹¤ë¥¸ ë©”ëª¨ë¦¬ ì¡°ê±´ì— ë”°ë¼ ê°’ë“¤ì´ ë³€í•œë‹¤ëŠ” ê²ƒì´ë‹¤. ì´ëŸ° ì´ìœ ë¡œ ì¸í•´ ê¸°ì¡´ì˜ ë°©ë²•ë“¤ì€ í•˜ë‚˜ì˜ ê°’ë§Œ ì¡°ì ˆí•˜ëŠ” ê¸°ë²•ì„ ì‚¬ìš©í–ˆë‹¤. 3ê°€ì§€ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ì˜í–¥ì„ ë¯¸ì¹œë‹¤.</p>

    <ol>
      <li>
        <ul>
          <li><strong>Depth</strong> : ë„¤íŠ¸ì›Œí¬ì˜ depthê°€ ì»¤ì§ˆ ìˆ˜ë¡, ë” ë³µì¡í•œ featureë¥¼ capture í•  ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ vanishing graident ë¬¸ì œê°€ ë°œìƒí•œë‹¤.</li>
        </ul>
      </li>
      <li>
        <ul>
          <li><strong>Width</strong> : ë„¤íŠ¸ì›Œí¬ì˜ widthê°€ ë„“ì–´ì§ˆ ìˆ˜ë¡, ë” ë¯¸ì„¸í•œ featureë¥¼ capture í•  ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ depthê°€ ì¶©ë¶„íˆ ê¹Šì§€ ì•Šë‹¤ë©´, ì¶”ìƒì •ë³´(high-level features)ë¥¼ íšë“í•˜ê¸° ì–´ë µë‹¤.</li>
        </ul>
      </li>
      <li>
        <ul>
          <li><strong>Resolution</strong> : ì…ë ¥ ì˜ìƒì˜ í•´ìƒë„ê°€ í´ ìˆ˜ë¡, ë” ì •ë°€í•œ íŒ¨í„´ì„ capture í•  ìˆ˜ ìˆë‹¤.</li>
        </ul>
      </li>
    </ol>
  </li>
</ul>

<p align="center"><img src="https://drive.google.com/uc?id=1FnnOo3yfR0FnXrDOA2w3QZxRTjd3BWd9" width="100%" height="100%" /></p>

<ul>
  <li>Figure3ì—ì„œ ì²˜ëŸ¼ ê°ê° íŒŒë¼ë¯¸í„°ì˜ ê°’ì„ ì˜¬ë¦´ ìˆ˜ë¡ ì„±ëŠ¥ì´ ì˜¬ë¼ê°„ë‹¤. í•˜ì§€ë§Œ ì–´ëŠì •ë„ ì˜¬ë¼ê°€ë©´ ì„±ëŠ¥ì´ ìˆ˜ë ´í•˜ê²Œ ëœë‹¤(=accuracy gainì´ ì ì–´ì§„ë‹¤).</li>
</ul>

<h3 id="compound-scaling">Compound Scaling</h3>

<p align="center"><img src="https://drive.google.com/uc?id=1Q86GB1Q99Y-oNMjkz2g6bWTtI9qkTC-I" width="70%" height="100%" /></p>

<blockquote>
  <p>In order to pursue better accuracy and efficiency, it is critical to balance all dimensions of network width, depth, and resolution during ConvNet scaling.</p>
</blockquote>

<ul>
  <li>ì§ê´€ì ìœ¼ë¡œ, ì…ë ¥ ì´ë¯¸ì§€ì˜ í•´ìƒë„ê°€ ì»¤ì§€ë©´ ë” ë§ì€ í”½ì…€ì •ë³´ë¥¼ ë‹´ê¸° ìœ„í•´ì„œ ëª¨ë¸ì˜ depth, widthë„ ì»¤ì ¸ì•¼ í•œë‹¤. ì´ì— ëŒ€í•œ ì‹¤í—˜ ê²°ê³¼ëŠ” Figure4ì™€ ê°™ë‹¤. depth, resolutionì„ ê°ê° ì¡°ì ˆí•˜ëŠ” ê²ƒ ë³´ë‹¤, ë‘˜ ë‹¤ ì¡°ì ˆí•˜ëŠ” ê²ƒì´ ì„±ëŠ¥ì´ ë” ì¢‹ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.</li>
</ul>

<p align="center"><img src="https://drive.google.com/uc?id=1bASUuxuC7pzMLZbFRLTYjZeLuhRn1dGn" width="70%" height="100%" /></p>

<h2 id="efficientnet-architecture">EfficientNet Architecture</h2>
<hr />

<h2 id="experiments">Experiments</h2>
<hr />

<h2 id="conclusion">Conclusion</h2>
<hr />
:ET